# --- –ò–ú–ü–û–†–¢–´ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û ---
import os
import asyncio
import random
import re
import httpx
from typing import List, Dict, Optional, Union
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

# from utils.logger import TCPLogger


# ====== –ò–ó–ú–ï–ù–Å–ù–ù–´–ô __init__ ======
class ShortsParser:
    def __init__(
            self,
            # logger: TCPLogger,
            youtube_api_key: str | None = None,
            api_base: str = "https://cosmeya.dev-klick.cyou/api/v1",
            yt_quota_sleep: float = 0.25
    ):
        # self.logger = logger
        self.current_proxy_index = 0
        self.seen_video_ids: set = set()
        self.collected_videos: List[Dict] = []
        self.response_tasks: List[asyncio.Task] = []
        self.dom_images = {}
        # NEW:
        self.youtube_api_key = youtube_api_key or os.getenv("YT_API_KEY")
        self.api_base = api_base.rstrip("/")
        self.yt_quota_sleep = yt_quota_sleep

    # ====== –£–¢–ò–õ–ò–¢–ê –†–ê–ó–ë–ò–í–ö–ò –ù–ê –ë–ê–¢–ß–ò ======
    def _chunked(self, seq, n):
        for i in range(0, len(seq), n):
            yield seq[i:i+n]

    async def extract_images_from_dom(self, page, url: str):
        """–ö–∞–∫ –≤ –∫–æ–¥–µ 2: –∏–¥—ë–º –ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º, –±–µ—Ä—ë–º href -> video_id –∏ img.src/srcset.
        –ê–∫–∫—É–º—É–ª–∏—Ä—É–µ–º –≤ self.dom_images (–Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º). –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª-–≤–æ –∫–∞—Ä—Ç–∏–Ω–æ–∫."""
        print("üîç –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ DOM (–ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º, –∫–∞–∫ –≤ –∫–æ–¥–µ 2)...")

        item_selectors = [
            "ytm-shorts-lockup-view-model",   # –º–æ–±–∏–ª—å–Ω–∞—è
            "ytd-rich-item-renderer",         # –¥–µ—Å–∫—Ç–æ–ø–Ω–∞—è
            "ytd-reel-item-renderer",         # reel items
            "ytd-grid-video-renderer"         # —Å–µ—Ç–∫–∞
        ]

        added = 0
        total_cards_seen = 0

        for selector in item_selectors:
            try:
                items = await page.query_selector_all(selector)
                total_cards_seen += len(items)
                print(f"–ö–∞—Ä—Ç–æ—á–µ–∫ –ø–æ '{selector}': {len(items)}")

                for el in items:
                    try:
                        # 1) —Å—Å—ã–ª–∫–∞ –Ω–∞ —à–æ—Ä—Ç ‚Äî –¥–æ—Å—Ç–∞—ë–º video_id –∏–∑ href
                        link_el = await el.query_selector("a[href*='/shorts/']") \
                                or await el.query_selector("a.shortsLockupViewModelHostEndpoint")
                        href = await link_el.get_attribute("href") if link_el else None
                        if not href:
                            continue
                        m = re.search(r"/shorts/([a-zA-Z0-9_-]{11})", href)
                        if not m:
                            continue
                        video_id = m.group(1)

                        img_el = await el.query_selector("img.ytCoreImageHost, img.yt-img-shadow, img")
                        img_url = None
                        if img_el:
                            src = await img_el.get_attribute("src")
                            if src and src.strip() and not src.startswith("data:"):
                                img_url = src
                            else:
                                # –±—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç–æ–ª—å–∫–æ srcset
                                srcset = await img_el.get_attribute("srcset")
                                if srcset:
                                    parts = [p.strip().split(" ")[0] for p in srcset.split(",") if p.strip()]
                                    if parts:
                                        img_url = parts[-1]

                        if not img_url:
                            img_url = f"https://i.ytimg.com/vi/{video_id}/hqdefault.jpg"

                        if video_id not in self.dom_images:
                            self.dom_images[video_id] = img_url
                            added += 1

                    except Exception:
                        continue

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ö–æ–¥–µ '{selector}': {e}")
                continue

        print(f"‚úÖ self.dom_images –ø–æ–ø–æ–ª–Ω–µ–Ω: +{added}, –≤—Å–µ–≥–æ: {len(self.dom_images)}; –∫–∞—Ä—Ç–æ—á–µ–∫ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ: {total_cards_seen}")
        return len(self.dom_images)

    async def scroll_until(
            self,
            page,
            url: str,
            selector: str,
            delay: float = 4.0,
            max_idle_rounds: int = 5,
            max_total_scrolls: int = 60,
    ):
        """
        –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É, –ø–æ–∫–∞ –Ω–µ –ø–µ—Ä–µ—Å—Ç–∞–Ω—É—Ç –ø–æ—è–≤–ª—è—Ç—å—Å—è –Ω–æ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏.
        –î–æ–±–∞–≤–ª–µ–Ω –≤–µ—Ä—Ö–Ω–∏–π –ø–æ—Ç–æ–ª–æ–∫ –ø–æ –æ–±—â–µ–º—É —á–∏—Å–ª—É –ø—Ä–æ–∫—Ä—É—Ç–æ–∫, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ü–∏–∫–ª–∏—Ç—å—Å—è.
        """
        max_scroll_attempts = 3
        total_scrolls = 0
        idle_rounds = 0

        try:
            prev_count = await page.eval_on_selector_all(selector, "els => els.length")
        except PlaywrightTimeoutError:
            prev_count = 0

        # –°–Ω–∏–º–∞–µ–º –ø–µ—Ä–≤—ã–π —Å—Ä–µ–∑ DOM –¥–æ –Ω–∞—á–∞–ª–∞ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∫—Ä–æ–ª–ª–∞
        await self.extract_images_from_dom(page, url)

        reached_bottom = False

        for attempt in range(1, max_scroll_attempts + 1):
            print(f"–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_scroll_attempts}")

            while total_scrolls < max_total_scrolls:
                total_scrolls += 1

                # –ò–º–∏—Ç–∞—Ü–∏—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∏ –∫–æ–ª–µ—Å–æ–º –º—ã—à–∏ –ø–æ–º–æ–≥–∞–µ—Ç YouTube –ø–æ–¥–≥—Ä—É–∂–∞—Ç—å –Ω–æ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏
                try:
                    await page.mouse.wheel(0, random.randint(600, 900))
                except Exception as e:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∫—Ä—É—Ç–∏—Ç—å –∫–æ–ª–µ—Å–æ–º –º—ã—à–∏: {e}. –ü—Ä–æ–±—É–µ–º scrollBy.")
                    await page.evaluate("distance => window.scrollBy(0, distance)", 1000)

                await page.wait_for_timeout(int(delay * 1000))

                captcha = await page.query_selector("text=CAPTCHA")
                if captcha:
                    print("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ CAPTCHA –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
                    return 0

                try:
                    current_count = await page.eval_on_selector_all(selector, "els => els.length")
                    print(f"–¢–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É '{selector}': {current_count}")
                except PlaywrightTimeoutError:
                    print("Timeout –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É...")
                    break

                if current_count > prev_count:
                    prev_count = current_count
                    idle_rounds = 0
                    await self.extract_images_from_dom(page, url)
                    try:
                        await page.wait_for_load_state("networkidle", timeout=int(delay * 1000))
                    except PlaywrightTimeoutError:
                        pass
                else:
                    idle_rounds += 1
                    if idle_rounds >= max_idle_rounds:
                        print(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞ –≤–∏–¥–µ–æ –ø—Ä–æ—Ñ–∏–ª—è {url}")
                        reached_bottom = True
                        break

                is_at_bottom = await page.evaluate(
                    "() => (window.innerHeight + window.scrollY) >= document.body.scrollHeight - 100"
                )
                if is_at_bottom:
                    reached_bottom = True
                    break

            if reached_bottom:
                break

            if total_scrolls >= max_total_scrolls:
                print("‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ø—Ä–æ–∫—Ä—É—Ç–æ–∫, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è.")
                break

            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π: –∏–Ω–æ–≥–¥–∞ YouTube –¥–æ–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π
            idle_rounds = 0
            await page.wait_for_timeout(1500)

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ —Å–∫—Ä–æ–ª–ª–∞
        await self.extract_images_from_dom(page, url)
        return len(self.dom_images)

    # ====== –ù–û–í–û–ï: –∑–∞–ø—Ä–æ—Å –∫ YouTube Data API –ø–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–º ID ======
    async def fetch_youtube_meta(self, video_ids: List[str]) -> Dict[str, dict]:
        """
        –ë–µ—Ä—ë—Ç –±–∞—Ç—á–∞–º–∏ –ø–æ 50 ID –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç {id: item} –∏–∑ videos.list
        part=snippet,statistics => title, description, publishedAt, viewCount, likeCount, commentCount
        """
        if not self.youtube_api_key:
            raise RuntimeError("YouTube API –∫–ª—é—á –Ω–µ –∑–∞–¥–∞–Ω (youtube_api_key –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è YT_API_KEY).")

        out = {}
        fields = (
            "items("
            "id,"
            "snippet/publishedAt,"
            "snippet/title,"
            "snippet/description,"
            "statistics/viewCount,"
            "statistics/likeCount,"
            "statistics/commentCount)"
        )
        params_base = {
            "part": "snippet,statistics",
            "key": self.youtube_api_key,
            "fields": fields
        }

        async with httpx.AsyncClient(timeout=30.0) as client:
            for batch in self._chunked(video_ids, 50):
                try:
                    params = params_base | {"id": ",".join(batch)}
                    r = await client.get("https://www.googleapis.com/youtube/v3/videos", params=params)
                    if r.status_code != 200:
                        print(f"‚ö†Ô∏è videos.list {r.status_code}: {r.text[:200]}")
                        await asyncio.sleep(self.yt_quota_sleep)
                        continue
                    data = r.json()
                    for it in data.get("items", []):
                        out[it["id"]] = it
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ videos.list: {e}")
                await asyncio.sleep(self.yt_quota_sleep)  # –±–µ—Ä–µ–∂—ë–º –∫–≤–æ—Ç—É
        return out

    # ====== –ò–ó–ú–ï–ù–Å–ù–ù–´–ô upload_image: –∏—Å–ø–æ–ª—å–∑—É–µ–º api_base ======

    async def download_image(self, url: str, proxy: str = None) -> Union[bytes, None]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å YouTube (–º–æ–∂–Ω–æ —Å –ø—Ä–æ–∫—Å–∏)."""
        try:
            if proxy and not proxy.startswith(("http://", "https://")):
                proxy = "http://" + proxy
            async with httpx.AsyncClient(proxy=proxy, timeout=20.0) as client:
                resp = await client.get(url)
                resp.raise_for_status()
                return resp.content
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {url}: {e}")
            return None

    async def upload_image(self, video_id: int, image_url: str, proxy: str = None):
        image_bytes = await self.download_image(image_url, proxy=proxy)
        if not image_bytes:
            return None, "Download failed"

        file_name = image_url.split("/")[-1].split("?")[0] or "cover.jpg"
        async with httpx.AsyncClient(timeout=30.0) as client:
            files = {"file": (file_name, image_bytes, "image/jpeg")}
            try:
                resp = await client.post(
                    f"{self.api_base}/videos/{video_id}/upload-image/",
                    files=files,
                )
                resp.raise_for_status()
                return resp.status_code, resp.text
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–æ—Ç–æ –¥–ª—è –≤–∏–¥–µ–æ {video_id}: {e}")
                return None, str(e)

    # ====== NEW: —Å–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ payload –∏–∑ API –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ –≤–∞—à –±—ç–∫–µ–Ω–¥ ======
    async def upsert_videos_to_backend(self, payloads: List[Dict], proxy_list: list | None = None) -> tuple[int, List[tuple[int, str]]]:
        """
        –°–æ–∑–¥–∞—ë–º/–æ–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –Ω–∞ –≤–∞—à–∏—Ö —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞—Ö.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (processed_count, image_queue), –≥–¥–µ image_queue = [(video_id, image_url), ...]
        """
        processed_count = 0
        image_queue: List[tuple[int, str]] = []

        async with httpx.AsyncClient(timeout=20.0) as client:
            for video_data in payloads:
                try:
                    # –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –∑–∞–ø–∏—Å—å –ø–æ link
                    check_resp = await client.get(f"{self.api_base}/videos/", params={"link": video_data["link"]})
                    is_new = False
                    video_id = None

                    if check_resp.status_code == 200:
                        res = check_resp.json()
                        vids = res.get("videos", [])
                        if vids:
                            video_id = vids[0]["id"]
                            # –ü–∞–∫–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                            patch_data = {
                                "amount_views": video_data["amount_views"],
                                "amount_likes": video_data["amount_likes"],
                                "amount_comments": video_data["amount_comments"],
                                "date_published": video_data["date_published"],
                            }
                            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ–±—Ä–æ—Å–∏—Ç—å description, –µ—Å–ª–∏ –±—ç–∫–µ–Ω–¥ —ç—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç
                            if video_data.get("description") is not None:
                                patch_data["description"] = video_data["description"]
                            try:
                                await client.patch(f"{self.api_base}/videos/{video_id}", json=patch_data)
                            except httpx.HTTPStatusError as e:
                                # –µ—Å–ª–∏ 4xx –∏–∑-–∑–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ –ø–æ–ª—è, –ø–æ–≤—Ç–æ—Ä–∏–º –±–µ–∑ description
                                if e.response is not None and e.response.status_code in (400, 422) and "description" in patch_data:
                                    patch_data.pop("description", None)
                                    await client.patch(f"{self.api_base}/videos/{video_id}", json=patch_data)
                        else:
                            is_new = True
                    else:
                        is_new = True

                    if is_new:
                        # –ü–æ–ª–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ
                        create_data = video_data.copy()
                        try:
                            resp = await client.post(f"{self.api_base}/videos/", json=create_data)
                            resp.raise_for_status()
                            video_id = resp.json()["id"]
                        except httpx.HTTPStatusError as e:
                            # fallback –±–µ–∑ description, –µ—Å–ª–∏ –ø–æ–ª–µ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
                            if e.response is not None and e.response.status_code in (400, 422) and "description" in create_data:
                                create_data.pop("description", None)
                                resp = await client.post(f"{self.api_base}/videos/", json=create_data)
                                resp.raise_for_status()
                                video_id = resp.json()["id"]
                            else:
                                raise

                        # –ø–ª–∞–Ω–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –æ–±–ª–æ–∂–∫–∏
                        if video_data.get("image"):
                            image_queue.append((video_id, video_data["image"]))

                    processed_count += 1

                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {video_data.get('link')}: {e}")

        return processed_count, image_queue

    def generate_short_title(self, full_title: str, max_length: int = 30) -> str:
        if not full_title:
            return ""
        if len(full_title) <= max_length:
            return full_title
        truncated = full_title[:max_length]
        last_space = truncated.rfind(' ')
        if last_space != -1:
            return truncated[:last_space]
        return truncated

    def extract_article_tag(self, caption: str) -> Optional[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É —Å–æ –í–°–ï–ú–ò –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏ (#sv, #jw –∏ —Ç.–¥.) —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –∏–ª–∏ None."""
        if not caption:
            return None

        allowed_tags = ["#sv", "#jw", "#qz", "#sr", "#fg"]
        found_tags = []

        caption_lower = caption.lower()
        original_caption = caption  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è

        for tag in allowed_tags:
            if tag in caption_lower:
                # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                start = caption_lower.find(tag)
                if start != -1:
                    # –ë–µ—Ä—ë–º —Ç–æ—á–Ω–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –∫—Ç–æ-—Ç–æ –Ω–∞–ø–∏—Å–∞–ª #SV)
                    exact_tag = original_caption[start:start + len(tag)]
                    found_tags.append(exact_tag)

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        found_tags = sorted(set(found_tags))

        return ",".join(found_tags) if found_tags else None

    # ====== –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î: —Å–∫—Ä–æ–ª–ª–∏–º /shorts ‚Üí —Å–æ–±–∏—Ä–∞–µ–º DOM ‚Üí YouTube API ‚Üí –±—ç–∫–µ–Ω–¥ ======
    async def parse_channel(self, url: str, channel_id: int, user_id: int,
                            max_retries: int = 3, proxy_list: list = None):
        """
        –ù–æ–≤–∞—è –ª–æ–≥–∏–∫–∞: —Ç–æ–ª—å–∫–æ DOM + YouTube API.
        - –°–∫—Ä–æ–ª–ª–∏–º /shorts, —Å–æ–±–∏—Ä–∞–µ–º {video_id: image_url}
        - –ë–∞—Ç—á–∞–º–∏ –¥–µ—Ä–≥–∞–µ–º YouTube Data API
        - –§–æ—Ä–º–∏—Ä—É–µ–º payload –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ –≤–∞—à–∏ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
        - –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–ª–æ–∂–∫–∏
        """
        self.proxy_list = proxy_list or []
        current_proxy_index = 0
        if not url.endswith("/shorts"):
            url = url.rstrip("/") + "/shorts"
        print(f"–ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –≤–∫–ª–∞–¥–∫—É Shorts: {url}")

        playwright = browser = context = page = None

        async def get_proxy_config(proxy_str):
            try:
                if "@" in proxy_str:
                    auth, host_port = proxy_str.split("@")
                    username, password = auth.split(":")
                    host, port = host_port.split(":")
                    return {"server": f"http://{host}:{port}", "username": username, "password": password}
                else:
                    host, port = proxy_str.split(":")
                    return {"server": f"http://{host}:{port}"}
            except Exception as e:
                print(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–∫—Å–∏: {e}")
                return None

        async def create_browser_with_proxy(proxy_str, playwright):
            proxy_config = await get_proxy_config(proxy_str) if proxy_str else None
            browser = await playwright.chromium.launch(
                headless=False,
                args=["--headless=new", "--disable-blink-features=AutomationControlled", "--start-maximized"],
            )
            context = await browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (X11; Linux x86_64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/138.0.0.0 Safari/537.36"
                ),
                viewport={"width": 1920, "height": 1080},
                proxy=proxy_config
            )
            page = await context.new_page()
            return browser, context, page

        current_proxy = random.choice(self.proxy_list) if self.proxy_list else None
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø—Ä–æ–∫—Å–∏: {current_proxy}")

        try:
            playwright = await async_playwright().start()
            browser, context, page = await create_browser_with_proxy(current_proxy, playwright)

            # 1) –û—Ç–∫—Ä—ã–≤–∞–µ–º /shorts –∏ —Å–∫—Ä–æ–ª–ª–∏–º, –ø–æ–ø—É—Ç–Ω–æ —Å–æ–±–∏—Ä–∞—è –∫–∞—Ä—Ç–æ—á–∫–∏
            await page.goto(url, wait_until="networkidle", timeout=60000)

            try:
                accept_btn = await page.query_selector("button[aria-label='Accept all']")
                if accept_btn:
                    await accept_btn.click()
                    await page.wait_for_timeout(1200)
                    print("–ó–∞–∫—Ä—ã—Ç–∞ –º–æ–¥–∞–ª–∫–∞ —Å –∫—É–∫–∏")
            except:
                pass

            selector = "ytd-rich-item-renderer, ytd-reel-item-renderer, ytm-shorts-lockup-view-model"
            total_videos_from_dom = await self.scroll_until(page, url, selector=selector, delay=4.0)
            if total_videos_from_dom == 0:
                print("‚ö†Ô∏è –ù–∞ –≤–∫–ª–∞–¥–∫–µ /shorts –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫")
                return []

            print(f"üìä –í DOM –Ω–∞–π–¥–µ–Ω–æ {total_videos_from_dom} –∫–∞—Ä—Ç–æ—á–µ–∫ Shorts")

            # 2) –°–æ–±–∏—Ä–∞–µ–º ID (–∫–ª—é—á–∏ —Å–ª–æ–≤–∞—Ä—è) –∏ –≤—ã–∑—ã–≤–∞–µ–º YouTube API
            video_ids = list(self.dom_images.keys())
            meta = await self.fetch_youtube_meta(video_ids)

            # 3) –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π payload
            #    (–æ–ø–∏—Å–∞–Ω–∏–µ, –ª–∞–π–∫–∏, –ø—Ä–æ—Å–º–æ—Ç—Ä—ã, –∫–æ–º–º–µ–Ω—Ç—ã, publishedAt ‚Äî –≤—Å—ë –∏–∑ API)
            all_videos_data = []
            for vid in video_ids:
                it = meta.get(vid)
                if not it:
                    continue
                sn = it.get("snippet", {}) or {}
                st = it.get("statistics", {}) or {}

                title = sn.get("title") or ""
                description = sn.get("description") or ""
                published_at = (sn.get("publishedAt") or "")[:10]  # YYYY-MM-DD
                view_count = int(st.get("viewCount")) if st.get("viewCount") is not None else 0
                like_count = int(st.get("likeCount")) if st.get("likeCount") is not None else 0
                comment_count = int(st.get("commentCount")) if st.get("commentCount") is not None else 0

                all_videos_data.append({
                    "link": f"https://www.youtube.com/shorts/{vid}",
                    "type": "youtube",
                    "name": self.generate_short_title(title),
                    "description": description,   # NEW
                    "image": self.dom_images.get(vid) or f"https://i.ytimg.com/vi/{vid}/hqdefault.jpg",
                    "articles": self.extract_article_tag(title),
                    "channel_id": channel_id,
                    "amount_views": view_count,
                    "amount_likes": like_count,
                    "amount_comments": comment_count,
                    "date_published": published_at
                })

            print(f"‚úÖ –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω payload –ø–æ {len(all_videos_data)} –≤–∏–¥–µ–æ")

        except Exception as main_error:
            print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {main_error}")
            raise
        finally:
            for obj, name in [(page, "page"), (context, "context"), (browser, "browser"), (playwright, "playwright")]:
                if obj:
                    try:
                        if name == "playwright":
                            await obj.stop()
                        else:
                            await obj.close()
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è {name}: {e}")

        # 4) –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –≤–∞—à –±—ç–∫–µ–Ω–¥ –∏ –ø–ª–∞–Ω–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –æ–±–ª–æ–∂–µ–∫
        processed_count, image_queue = await self.upsert_videos_to_backend(all_videos_data, proxy_list=proxy_list)
        print(f"üì¶ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –≤–∏–¥–µ–æ, –æ–∂–∏–¥–∞—é—Ç –∑–∞–≥—Ä—É–∑–∫–∏ {len(image_queue)} –æ–±–ª–æ–∂–µ–∫.")

        # 5) –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∫–∞–∫ —É –≤–∞—Å, —Ç–æ–ª—å–∫–æ base –±–µ—Ä—ë—Ç—Å—è –∏–∑ self.api_base)
        idx = 0
        while idx < len(image_queue):
            proxy = proxy_list[current_proxy_index] if proxy_list else None
            current_proxy_index = (current_proxy_index + 1) % len(proxy_list) if proxy_list else 0
            batch = image_queue[idx:idx + 15]
            print(f"üñºÔ∏è –ó–∞–≥—Ä—É–∂–∞–µ–º {len(batch)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ {proxy or '–±–µ–∑ –ø—Ä–æ–∫—Å–∏'}")

            for vid, img_url in batch:
                try:
                    await self.upload_image(vid, img_url, proxy=proxy)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–æ—Ç–æ {vid}: {e}")
                await asyncio.sleep(5.0)
            idx += 15

        print(f"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω: {processed_count} –≤–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ.")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∫–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return all_videos_data


# # ----------------------- –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ -----------------------

async def main():
    proxy_list = [
        "g3dmsMyYST:B9BegRNRzi@45.150.35.224:28898",
        "Weh1oXn82b:dUYiJZ5w7T@45.150.35.129:31801",
        "gnmPrWSMJ4:tbHyXTwWdx@45.150.35.114:54943",
        "15ObFJmCP5:a0rog6kGgT@45.150.35.113:24242",
        "Z7mGFwrT6N:5wLFFO5v3S@109.120.131.5:34707",
        "HCtCUxQYnj:GM9pjQ8J8T@109.120.131.229:39202",
        "dBY505zGKK:8gqxiwpjvg@45.150.35.44:40281",
        "zhH47betn3:J8eC3qaOrs@109.120.131.175:38411",
        "KX32alVE51:ZVD0CsjFhJ@109.120.131.27:47449",
        "KTdw9aNBl7:MI45E5jVnB@45.150.35.233:57281",
        "7bZbeHwcNI:fFs1cUXfbN@109.120.131.219:29286",
        "F1Y0BvrqNo:HKPbfMGtJw@45.150.35.31:41247",
        "WfkB8GfYts:vXdJAVXCSI@45.150.35.133:35460",
        "yr3Xib8LYo:FzS9t4PGro@45.150.35.3:50283",
        "exOL0CR6TN:oj0BGarhAk@45.150.35.143:32354",
        "CbZ35SQIZb:OO4ddjBRiK@45.150.35.99:28985",
        "JRGI3q6Zo9:LJpcFpCgU2@45.150.35.30:32381",
        "NTPvsl77eN:wagp6GmWNk@109.120.131.41:55509",
        "SBqj98lU9c:ktxTU1ZOid@45.150.35.138:55350",
        "3El7Uvg1TY:1DZVyrdMPs@45.150.35.231:51842",
        "dBqOOqGczg:d2xKkdc3Re@45.150.35.156:38617",
        "fz91O4ury3:ZBCW6s8d7E@45.150.35.132:47712",
        "RLFUp7vicq:X1TTYhQYWs@45.150.35.34:40674",
        "3dQxPpHkj4:o12oWKn5Lg@45.150.35.201:42897",
        "iRArjOVFVr:0vXB48RsTf@45.150.35.200:42312",
    ]
    parser = ShortsParser()
    url = "https://www.youtube.com/@nastya.beomaa"
    user_id = 1
    await parser.parse_channel(url, channel_id=4, user_id=user_id,
                               proxy_list=proxy_list)


if __name__ == "__main__":
    asyncio.run(main())
