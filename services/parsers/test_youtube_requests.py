import re
import asyncio
# import time
import json
from typing import Optional, Dict, List, Union, Any
import httpx
import requests
# from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
import random
from datetime import datetime

from bs4 import BeautifulSoup


# from utils.logger import TCPLogger


class ShortsParser:
    def __init__(
            self,
            # logger: TCPLogger
    ):
        # self.logger = logger
        self.current_proxy_index = 0
        self.seen_video_ids: set = set()
        self.collected_videos: List[Dict] = []
        self.response_tasks: List[asyncio.Task] = []
        self.dom_images = {}
        self.dom_video_links = {}
        self.dom_order: List[str] = []
        self.saved_html_count = 0

    def reset_dom_state(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ DOM-–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –Ω–æ–≤–æ–π –ø–æ–ø—ã—Ç–∫–æ–π –ø–∞—Ä—Å–∏–Ω–≥–∞."""
        self.dom_images = {}
        self.dom_video_links = {}
        self.dom_order = []
        self.collected_videos.clear()
        self.seen_video_ids.clear()
        self.response_tasks.clear()
        self.saved_html_count = 0

    def parse_views(self, text: str) -> int:
        if not text:
            return 0
        cleaned = text.replace("\xa0", " ").strip()
        match = re.search(r"([\d\s.,]+)", cleaned)
        if not match:
            return 0
        number_part = match.group(1)
        digits_only = re.sub(r"[^\d]", "", number_part)
        return int(digits_only) if digits_only else 0

    def parse_compact_number(self, raw_number: str, suffix: Optional[str] = None) -> Optional[int]:
        if not raw_number:
            return None

        cleaned = raw_number.replace("\xa0", "").replace(" ", "")
        cleaned = cleaned.replace(",", ".")

        try:
            value = float(cleaned)
        except ValueError:
            return None

        if suffix:
            suffix_normalized = suffix.strip().lower()
            if suffix_normalized in {"k", "—Ç—ã—Å"}:
                value *= 1_000
            elif suffix_normalized in {"m", "–º–ª–Ω"}:
                value *= 1_000_000
            elif suffix_normalized in {"b", "–º–ª—Ä–¥"}:
                value *= 1_000_000_000

        return int(round(value))

    async def get_videos_count_from_header(self, page, timeout: int = 8000) -> Optional[int]:
        try:
            try:
                await page.wait_for_selector("yt-content-metadata-view-model span", timeout=timeout)
            except PlaywrightTimeoutError:
                pass

            header_elements = await page.query_selector_all("yt-content-metadata-view-model span")
            for element in header_elements:
                try:
                    raw_text = await element.inner_text()
                except Exception:
                    continue

                if not raw_text:
                    continue

                normalized = re.sub(r"\s+", " ", raw_text).strip()
                lowered = normalized.lower()

                if "video" not in lowered and "–≤–∏–¥–µ–æ" not in lowered:
                    continue

                match = re.search(r"([\d\s.,]+)\s*(k|m|b|—Ç—ã—Å|–º–ª–Ω|–º–ª—Ä–¥)?", normalized, re.IGNORECASE)
                if not match:
                    continue

                number_part = match.group(1)
                suffix = match.group(2)
                parsed = self.parse_compact_number(number_part, suffix)
                if parsed:
                    return parsed
        except Exception as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –∏–∑ —à–∞–ø–∫–∏: {e}")

        return None

    async def extract_images_from_dom(self, page, url: str):
        """–ü—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –∏ –ø—Ä–µ–≤—å—é –¥–ª—è —à–æ—Ä—Ç–æ–≤."""
        print("üîç –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —à–æ—Ä—Ç–∞—Ö –∏–∑ DOM‚Ä¶")

        item_selectors = [
            "ytm-shorts-lockup-view-model",   # –º–æ–±–∏–ª—å–Ω–∞—è
            "ytd-rich-item-renderer",         # –¥–µ—Å–∫—Ç–æ–ø–Ω–∞—è
            "ytd-reel-item-renderer",         # reel items
            "ytd-grid-video-renderer"         # —Å–µ—Ç–∫–∞
        ]

        added_images = 0
        added_links = 0
        total_cards_seen = 0

        for selector in item_selectors:
            try:
                items = await page.query_selector_all(selector)
                total_cards_seen += len(items)
                print(f"–ö–∞—Ä—Ç–æ—á–µ–∫ –ø–æ '{selector}': {len(items)}")

                for el in items:
                    try:
                        link_el = await el.query_selector("a[href*='/shorts/']") \
                                or await el.query_selector("a.shortsLockupViewModelHostEndpoint")
                        href = await link_el.get_attribute("href") if link_el else None
                        if not href:
                            continue
                        m = re.search(r"/shorts/([a-zA-Z0-9_-]{11})", href)
                        if not m:
                            continue
                        video_id = m.group(1)

                        if video_id not in self.dom_video_links:
                            self.dom_video_links[video_id] = f"https://www.youtube.com/shorts/{video_id}"
                            self.dom_order.append(video_id)
                            added_links += 1

                        img_el = await el.query_selector("img.ytCoreImageHost, img.yt-img-shadow, img")
                        img_url = None
                        if img_el:
                            src = await img_el.get_attribute("src")
                            if src and src.strip() and not src.startswith("data:"):
                                img_url = src
                            else:
                                # –±—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç–æ–ª—å–∫–æ srcset
                                srcset = await img_el.get_attribute("srcset")
                                if srcset:
                                    parts = [p.strip().split(" ")[0] for p in srcset.split(",") if p.strip()]
                                    if parts:
                                        img_url = parts[-1]

                        if not img_url:
                            img_url = f"https://i.ytimg.com/vi/{video_id}/hqdefault.jpg"

                        if video_id not in self.dom_images or not self.dom_images[video_id]:
                            self.dom_images[video_id] = img_url
                            added_images += 1

                    except Exception:
                        continue

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ö–æ–¥–µ '{selector}': {e}")
                continue

        print(
            f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ: +{added_links} —Å—Å—ã–ª–æ–∫, +{added_images} –ø—Ä–µ–≤—å—é; –≤—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ: "
            f"{len(self.dom_order)}; –∫–∞—Ä—Ç–æ—á–µ–∫ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ: {total_cards_seen}"
        )
        return len(self.dom_order)

    async def scroll_until(self, page, url: str, selector: str, target_count: Optional[int] = None,
                           delay: float = 2.5, max_idle_rounds: int = 7):
        """–°–∫—Ä–æ–ª–ª–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É, –ø–æ–∫–∞ –Ω–µ —Å–æ–±–µ—Ä—ë–º –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–æ—Ä—Ç–æ–≤ –∏–ª–∏ –Ω–µ –¥–æ–π–¥—ë–º –¥–æ –∫–æ–Ω—Ü–∞."""
        prev_count = len(self.dom_order)
        idle_rounds = 0
        max_scroll_attempts = 6

        for attempt in range(max_scroll_attempts):
            print(f"–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_scroll_attempts}")

            if attempt > 0:
                try:
                    await page.evaluate("() => window.scrollTo({top: 0, behavior: 'instant'})")
                    await page.wait_for_timeout(800)
                except Exception:
                    pass

            while True:
                prev_height = await page.evaluate("() => document.documentElement.scrollHeight")

                try:
                    await page.keyboard.press("End")
                except Exception:
                    pass

                try:
                    await page.mouse.wheel(0, 1800)
                except Exception:
                    pass

                await page.wait_for_timeout(int(delay * 1000))

                height_increased = True
                try:
                    await page.wait_for_function(
                        "(oldHeight) => document.documentElement.scrollHeight - oldHeight > 120",
                        prev_height,
                        timeout=2500
                    )
                except PlaywrightTimeoutError:
                    height_increased = False
                except Exception:
                    height_increased = False

                captcha = await page.query_selector("text=CAPTCHA")
                if captcha:
                    print("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ CAPTCHA –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
                    return len(self.dom_order)

                await self.extract_images_from_dom(page, url)

                current_total = len(self.dom_order)
                target_info = target_count if target_count else "?"
                print(f"üî¢ –°–æ–±—Ä–∞–Ω–æ {current_total} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ (—Ü–µ–ª—å: {target_info})")

                if target_count and current_total >= target_count:
                    print("üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Ç—Ä–µ–±—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –∏–∑ —à–∞–ø–∫–∏.")
                    return current_total

                try:
                    current_count = await page.eval_on_selector_all(selector, "els => els.length")
                    print(f"–¢–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É '{selector}': {current_count}")
                except PlaywrightTimeoutError:
                    print("Timeout –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")

                if current_total == prev_count and not height_increased:
                    idle_rounds += 1
                    if idle_rounds >= max_idle_rounds:
                        print(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞ –≤–∏–¥–µ–æ –ø—Ä–æ—Ñ–∏–ª—è {url}")
                        return current_total
                else:
                    idle_rounds = 0
                    prev_count = current_total

                # –µ—Å–ª–∏ –≤—ã—Å–æ—Ç–∞ –Ω–µ —É–≤–µ–ª–∏—á–∏–ª–∞—Å—å –∏ –º—ã –≤—Å—ë –µ—â—ë –≤–Ω–∏–∑—É ‚Äî –¥–µ–ª–∞–µ–º –Ω–µ–±–æ–ª—å—à—É—é –ø–∞—É–∑—É
                if not height_increased:
                    await page.wait_for_timeout(800)

                if height_increased:
                    continue

                # –ø—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—è–≤–∏–ª—Å—è –ª–∏ –Ω–æ–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
                try:
                    newly_visible = await page.eval_on_selector_all(
                        selector,
                        "els => els.length"
                    )
                except PlaywrightTimeoutError:
                    newly_visible = None

                if newly_visible is not None and newly_visible <= current_total:
                    break

        await self.extract_images_from_dom(page, url)
        return len(self.dom_order)

    def prepare_proxy(self, proxy_str: Optional[str]) -> Optional[str]:
        """–ü—Ä–∏–≤–æ–¥–∏–º —Å—Ç—Ä–æ–∫—É –ø—Ä–æ–∫—Å–∏ –∫ —Ñ–æ—Ä–º–∞—Ç—É, –ø–æ–Ω—è—Ç–Ω–æ–º—É requests."""
        if not proxy_str:
            return None
        proxy_str = proxy_str.strip()
        if not proxy_str:
            return None
        if proxy_str.startswith(("http://", "https://")):
            return proxy_str
        if "@" in proxy_str:
            auth, host_port = proxy_str.split("@", 1)
            host, port = host_port.split(":", 1)
            return f"http://{auth}@{host}:{port}"
        if ":" in proxy_str:
            host, port = proxy_str.split(":", 1)
            return f"http://{host}:{port}"
        return proxy_str

    def _extract_json_fragment(self, text: str, marker: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É, –Ω–∞—á–∏–Ω–∞—é—â—É—é—Å—è —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –º–∞—Ä–∫–µ—Ä–∞."""
        if marker not in text:
            return None
        start = text.find(marker)
        if start == -1:
            return None
        start += len(marker)
        while start < len(text) and text[start] in " \n\r\t=":
            start += 1
        if start >= len(text):
            return None
        opening = text[start]
        if opening not in "{[":
            return None
        closing = "}" if opening == "{" else "]"
        depth = 0
        in_string = False
        escape = False

        for pos in range(start, len(text)):
            ch = text[pos]
            if in_string:
                if escape:
                    escape = False
                elif ch == "\\":
                    escape = True
                elif ch == '"':
                    in_string = False
                continue

            if ch == '"':
                in_string = True
            elif ch == opening:
                depth += 1
            elif ch == closing:
                depth -= 1
                if depth == 0:
                    return text[start:pos + 1]

        return None

    def _load_json_segment(self, text: str, markers: List[str]) -> Optional[Dict[str, Any]]:
        for marker in markers:
            fragment = self._extract_json_fragment(text, marker)
            if not fragment:
                continue
            try:
                return json.loads(fragment)
            except json.JSONDecodeError as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –ø–æ –º–∞—Ä–∫–µ—Ä—É '{marker}': {e}")
        return None

    def parse_video_page(self, html: str) -> Dict[str, Optional[Dict[str, Any]]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ytInitialPlayerResponse –∏ ytInitialData."""
        soup = BeautifulSoup(html, "html.parser")
        script_texts = []
        for script in soup.find_all("script"):
            if script.string:
                script_texts.append(script.string)

        player_markers = [
            "var ytInitialPlayerResponse = ",
            "ytInitialPlayerResponse = ",
            'window["ytInitialPlayerResponse"] = ',
            "window.ytInitialPlayerResponse = ",
        ]
        initial_markers = [
            "var ytInitialData = ",
            "ytInitialData = ",
            'window["ytInitialData"] = ',
            "window.ytInitialData = ",
        ]

        player_data = None
        initial_data = None

        for text in script_texts:
            if not player_data and "ytInitialPlayerResponse" in text:
                player_data = self._load_json_segment(text, player_markers)
            if not initial_data and "ytInitialData" in text:
                initial_data = self._load_json_segment(text, initial_markers)
            if player_data and initial_data:
                break

        # fallback ‚Äî –∏—â–µ–º –ø—Ä—è–º–æ –≤ html, –µ—Å–ª–∏ BeautifulSoup –Ω–µ –ø–æ–º–æ–≥
        if not player_data:
            player_data = self._load_json_segment(html, player_markers)
        if not initial_data:
            initial_data = self._load_json_segment(html, initial_markers)

        return {"player": player_data, "initial": initial_data}

    def extract_views_from_initial_data(self, data: Any) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä ytInitialData."""

        def parse_candidate(value: Any) -> Optional[int]:
            if isinstance(value, str):
                parsed = self.parse_views(value)
                return parsed if parsed else None
            if isinstance(value, dict):
                text = value.get("simpleText") or value.get("text")
                if text:
                    parsed = self.parse_views(text)
                    if parsed:
                        return parsed
                runs = value.get("runs")
                if runs:
                    combined = "".join(run.get("text", "") for run in runs if run.get("text"))
                    parsed = self.parse_views(combined)
                    if parsed:
                        return parsed
            return None

        stack = [data]
        visited = set()

        while stack:
            node = stack.pop()
            node_id = id(node)
            if node_id in visited:
                continue
            visited.add(node_id)

            if isinstance(node, dict):
                # videoDescriptionHeaderRenderer -> views / factoid
                header = node.get("videoDescriptionHeaderRenderer")
                if isinstance(header, dict):
                    direct_views = parse_candidate(header.get("views"))
                    if direct_views:
                        return direct_views

                    factoids = header.get("factoid")
                    if isinstance(factoids, list):
                        for fact in factoids:
                            renderer = fact.get("viewCountFactoidRenderer") if isinstance(fact, dict) else None
                            if isinstance(renderer, dict):
                                factoid_renderer = renderer.get("factoid", {}).get("factoidRenderer", {})
                                for key in ("accessibilityText", "value", "label"):
                                    candidate = factoid_renderer.get(key)
                                    parsed = parse_candidate(candidate)
                                    if parsed:
                                        return parsed
                                views_candidate = renderer.get("viewCount")
                                parsed = parse_candidate(views_candidate)
                                if parsed:
                                    return parsed

                # direct factoid structure without header wrapper
                renderer = node.get("viewCountFactoidRenderer")
                if isinstance(renderer, dict):
                    factoid_renderer = renderer.get("factoid", {}).get("factoidRenderer", {})
                    for key in ("accessibilityText", "value", "label"):
                        candidate = factoid_renderer.get(key)
                        parsed = parse_candidate(candidate)
                        if parsed:
                            return parsed
                    parsed = parse_candidate(renderer.get("viewCount"))
                    if parsed:
                        return parsed

                label = node.get("label")
                parsed_label = parse_candidate(label)
                if parsed_label:
                    for key in ("accessibilityText", "simpleText", "text", "title"):
                        candidate = node.get(key)
                        parsed = parse_candidate(candidate)
                        if parsed:
                            return parsed

                for value in node.values():
                    if isinstance(value, (dict, list)):
                        stack.append(value)

            elif isinstance(node, list):
                for item in node:
                    if isinstance(item, (dict, list)):
                        stack.append(item)

        return None

    async def fetch_video_metadata(self, video_id: str, video_url: str, proxy: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —à–æ—Ä—Ç–∞ –∏ –¥–æ—Å—Ç–∞—ë–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–Ω–∞–∑–≤–∞–Ω–∏–µ, –ø—Ä–æ—Å–º–æ—Ç—Ä—ã, –æ–ø–∏—Å–∞–Ω–∏–µ)."""
        formatted_proxy = self.prepare_proxy(proxy)
        # headers = {
        #     "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
        #     "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        # }
        proxies = {"http": formatted_proxy, "https": formatted_proxy} if formatted_proxy else None

        def _fetch_html() -> str:
            response = requests.get(
                video_url,
                # headers=headers,
                timeout=30.0,
                # allow_redirects=True,
                proxies=proxies,
            )
            response.raise_for_status()
            return response.text

        try:
            html = await asyncio.to_thread(_fetch_html)
        except Exception as e:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {video_url} —á–µ—Ä–µ–∑ {formatted_proxy or '–±–µ–∑ –ø—Ä–æ–∫—Å–∏'}: {e}")
            return None

        # if self.saved_html_count < 2:
        #     debug_dir = Path("debug_html")
        #     debug_dir.mkdir(parents=True, exist_ok=True)
        #     safe_video_id = video_id or f"video_{self.saved_html_count + 1}"
        #     debug_path = debug_dir / f"{self.saved_html_count + 1}_{safe_video_id}.html"
        #     try:
        #         debug_path.write_text(html)
        #         print(f"üíæ –°–æ—Ö—Ä–∞–Ω—ë–Ω HTML –æ—Ç–≤–µ—Ç–∞ {debug_path}")
        #     except Exception as save_err:
        #         print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML {debug_path}: {save_err}")
        #     finally:
        #         self.saved_html_count += 1

        parsed = self.parse_video_page(html)
        player_data = parsed.get("player") or {}
        if not player_data:
            print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω ytInitialPlayerResponse –¥–ª—è {video_url}")
            return None

        video_details = player_data.get("videoDetails", {}) or {}
        microformat_container = player_data.get("microformat", {})
        if not isinstance(microformat_container, dict):
            microformat_container = {}
        microformat = microformat_container.get("playerMicroformatRenderer", {}) or {}
        if not isinstance(microformat, dict):
            microformat = {}

        def _extract_text(value: Any) -> str:
            if isinstance(value, str):
                return value
            if isinstance(value, dict):
                if value.get("simpleText"):
                    return value["simpleText"]
                runs = value.get("runs")
                if isinstance(runs, list):
                    return "".join(run.get("text", "") for run in runs if isinstance(run, dict))
            return ""

        def _parse_number_from_text(raw: Optional[str]) -> Optional[int]:
            if not raw:
                return None
            cleaned = raw.replace("\xa0", " ").strip()
            match = re.search(
                r"([\d\s.,]+)\s*(—Ç—ã—Å(?:—è—á[–∞–∏])?|–º–ª–Ω|–º–∏–ª–ª–∏–æ–Ω(?:–æ–≤)?|–º–ª—Ä–¥|–º–∏–ª–ª–∏–∞—Ä–¥(?:–æ–≤)?|k|m|b)?",
                cleaned,
                re.IGNORECASE,
            )
            if match:
                number_part = match.group(1)
                suffix = match.group(2)
                if suffix:
                    suffix = suffix.strip().lower()
                    if suffix.startswith("—Ç—ã—Å"):
                        suffix = "—Ç—ã—Å"
                    elif suffix.startswith("–º–∏–ª–ª–∏–æ–Ω") or suffix == "–º–ª–Ω":
                        suffix = "–º–ª–Ω"
                    elif suffix.startswith("–º–∏–ª–ª–∏–∞—Ä–¥") or suffix == "–º–ª—Ä–¥":
                        suffix = "–º–ª—Ä–¥"
                parsed_number = self.parse_compact_number(number_part, suffix) if suffix else self.parse_views(number_part)
                if parsed_number:
                    return parsed_number
            return self.parse_views(cleaned)

        def _normalize_publish_date(value: Optional[str]) -> Optional[str]:
            if not value:
                return None
            candidate = value.strip()
            if candidate.endswith("Z"):
                candidate = candidate[:-1] + "+00:00"
            try:
                dt = datetime.fromisoformat(candidate)
                return dt.date().isoformat()
            except ValueError:
                return None

        def _extract_publish_date_from_factoids(factoids: List[Any]) -> Optional[str]:
            month_aliases = {
                "—è–Ω–≤": 1,
                "—è–Ω–≤–∞—Ä—å": 1,
                "—è–Ω–≤–∞—Ä—è": 1,
                "—Ñ–µ–≤": 2,
                "—Ñ–µ–≤—Ä": 2,
                "—Ñ–µ–≤—Ä–∞–ª—å": 2,
                "—Ñ–µ–≤—Ä–∞–ª—è": 2,
                "–º–∞—Ä": 3,
                "–º–∞—Ä—Ç": 3,
                "–º–∞—Ä—Ç–∞": 3,
                "–∞–ø—Ä": 4,
                "–∞–ø—Ä–µ–ª—å": 4,
                "–∞–ø—Ä–µ–ª—è": 4,
                "–º–∞–π": 5,
                "–º–∞—è": 5,
                "–∏—é–Ω": 6,
                "–∏—é–Ω—å": 6,
                "–∏—é–Ω—è": 6,
                "–∏—é–ª": 7,
                "–∏—é–ª—å": 7,
                "–∏—é–ª—è": 7,
                "–∞–≤–≥": 8,
                "–∞–≤–≥—É—Å—Ç": 8,
                "–∞–≤–≥—É—Å—Ç–∞": 8,
                "—Å–µ–Ω": 9,
                "—Å–µ–Ω—Ç": 9,
                "—Å–µ–Ω—Ç—è–±—Ä—å": 9,
                "—Å–µ–Ω—Ç—è–±—Ä—è": 9,
                "oct": 10,
                "october": 10,
                "–æ–∫—Ç": 10,
                "–æ–∫—Ç—è–±—Ä—å": 10,
                "–æ–∫—Ç—è–±—Ä—è": 10,
                "nov": 11,
                "november": 11,
                "–Ω–æ—è": 11,
                "–Ω–æ—è–±—Ä—å": 11,
                "–Ω–æ—è–±—Ä—è": 11,
                "dec": 12,
                "december": 12,
                "–¥–µ–∫": 12,
                "–¥–µ–∫–∞–±—Ä—å": 12,
                "–¥–µ–∫–∞–±—Ä—è": 12,
                "aug": 8,
                "august": 8,
                "apr": 4,
                "april": 4,
                "february": 2,
                "january": 1,
                "jan": 1,
                "feb": 2,
                "mar": 3,
                "may": 5,
                "jun": 6,
                "june": 6,
                "jul": 7,
                "july": 7,
                "sep": 9,
                "sept": 9,
                "september": 9,
            }
            for fact in factoids:
                if not isinstance(fact, dict):
                    continue
                renderer = fact.get("factoidRenderer")
                if not renderer:
                    renderer = fact.get("viewCountFactoidRenderer", {}).get("factoid", {}).get("factoidRenderer")
                if not renderer:
                    continue
                label_text = _extract_text(renderer.get("label"))
                value_text = _extract_text(renderer.get("value"))
                if not label_text or not value_text:
                    continue
                year_match = re.search(r"(\\d{4})", label_text)
                if not year_match:
                    continue
                year = int(year_match.group(1))
                day_match = re.search(r"(\\d{1,2})", value_text)
                month_match = re.search(r"([A-Za-z–ê-–Ø–∞-—è—ë–Å]+)", value_text)
                if not day_match or not month_match:
                    continue
                day = int(day_match.group(1))
                month_key = month_match.group(1).lower().rstrip(".")
                month = month_aliases.get(month_key)
                if not month:
                    continue
                try:
                    return datetime(year, month, day).date().isoformat()
                except ValueError:
                    continue
            return None

        title_candidate = video_details.get("title") or microformat.get("title")
        title = _extract_text(title_candidate)

        description = video_details.get("shortDescription")
        if not description:
            description = _extract_text(microformat.get("description"))
        description = description.strip() if isinstance(description, str) else ""

        view_count_raw = video_details.get("viewCount")
        views = 0
        if isinstance(view_count_raw, str) and view_count_raw.isdigit():
            views = int(view_count_raw)
        elif isinstance(view_count_raw, str):
            views = self.parse_views(view_count_raw)

        if not views:
            view_count_text = _extract_text(microformat.get("viewCount"))
            if view_count_text:
                views = self.parse_views(view_count_text)

        initial_data = parsed.get("initial") or {}
        overlay = {}
        if isinstance(initial_data, dict):
            overlay = initial_data.get("overlay", {}).get("reelPlayerOverlayRenderer", {}) or {}

        if not views and overlay:
            try:
                header = overlay.get("reelPlayerHeaderSupportedRenderers", {}).get("reelPlayerHeaderRenderer", {})
                sub_label = header.get("accessibility", {}).get("accessibilityData", {}).get("label", "")
                views = self.parse_views(sub_label)
            except Exception:
                views = views or 0

        if not views and initial_data:
            extracted = self.extract_views_from_initial_data(initial_data)
            if extracted:
                views = extracted

        likes = None
        microformat_like = microformat.get("likeCount")
        if isinstance(microformat_like, str):
            likes = self.parse_views(microformat_like)
        elif isinstance(microformat_like, (int, float)):
            likes = int(microformat_like)

        published_at = None
        for candidate in (microformat.get("publishDate"), microformat.get("uploadDate")):
            published_at = _normalize_publish_date(candidate)
            if published_at:
                break

        comments = None
        if overlay:
            button_bar = overlay.get("buttonBar", {}).get("reelActionBarViewModel", {})
            button_models = button_bar.get("buttonViewModels", []) if isinstance(button_bar, dict) else []
            for button in button_models:
                if not isinstance(button, dict):
                    continue
                like_vm = button.get("likeButtonViewModel")
                if like_vm:
                    like_count_vm = like_vm.get("likeCountViewModel")
                    if isinstance(like_count_vm, dict):
                        like_count_vm = like_count_vm.get("likeCountViewModel", like_count_vm)
                    if isinstance(like_count_vm, dict) and not likes:
                        like_candidate = like_count_vm.get("shortText") or like_count_vm.get("accessibilityText")
                        likes = _parse_number_from_text(like_candidate) or likes
                    if not likes:
                        toggle_vm = (
                            like_vm.get("toggleButtonViewModel", {})
                            .get("toggleButtonViewModel", {})
                            .get("defaultButtonViewModel", {})
                            .get("buttonViewModel", {})
                        )
                        like_text = toggle_vm.get("accessibilityText")
                        likes = _parse_number_from_text(like_text) or likes
                    continue

                generic_vm = button.get("buttonViewModel") or {}
                tooltip = generic_vm.get("tooltip") or generic_vm.get("title") or generic_vm.get("accessibilityText") or ""
                if isinstance(tooltip, str) and "–∫–æ–º–º–µ–Ω—Ç" in tooltip.lower():
                    raw_comments = generic_vm.get("title") or generic_vm.get("accessibilityText")
                    comments = _parse_number_from_text(raw_comments)

        if (published_at is None) and isinstance(initial_data, dict):
            try:
                panels = initial_data.get("engagementPanels", []) or []
                for panel in panels:
                    if not isinstance(panel, dict):
                        continue
                    section = panel.get("engagementPanelSectionListRenderer")
                    if not section:
                        continue
                    content = section.get("content", {})
                    structured = content.get("structuredDescriptionContentRenderer", {})
                    if not structured:
                        continue
                    for item in structured.get("items", []):
                        if not isinstance(item, dict):
                            continue
                        header = item.get("videoDescriptionHeaderRenderer")
                        if header:
                            published_at = _extract_publish_date_from_factoids(header.get("factoid", []))
                            if published_at:
                                break
                    if published_at:
                        break
            except Exception:
                published_at = published_at or None

        return {
            "video_id": video_id,
            "link": video_url,
            "title": title,
            "views": views or 0,
            "likes": likes or 0,
            "comments": comments or 0,
            "published_at": published_at or "",
            "description": description,
        }

    async def fetch_videos_with_proxies(self, video_ids: List[str], delay: float = 5.0) -> List[Dict[str, Any]]:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã —à–æ—Ä—Ç–æ–≤ –ø–∞–∫–µ—Ç–∞–º–∏, –ø–æ –æ–¥–Ω–æ–º—É URL –Ω–∞ –ø—Ä–æ–∫—Å–∏."""
        if not video_ids:
            return []

        proxies = self.proxy_list if self.proxy_list else [None]
        batch_size = len(proxies) if proxies else 1
        results: List[Dict[str, Any]] = []

        index = 0
        total = len(video_ids)
        while index < total:
            batch_ids = video_ids[index:index + batch_size]
            tasks: List[asyncio.Task] = []
            task_video_ids: List[str] = []

            for idx, video_id in enumerate(batch_ids):
                video_url = self.dom_video_links.get(video_id)
                if not video_url:
                    print(f"‚ö†Ô∏è –î–ª—è –≤–∏–¥–µ–æ {video_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –≤ DOM, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                    continue
                proxy = proxies[idx] if proxies else None
                task_video_ids.append(video_id)
                tasks.append(asyncio.create_task(self.fetch_video_metadata(video_id, video_url, proxy)))

            if tasks:
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                for video_id, result in zip(task_video_ids, batch_results):
                    if isinstance(result, Exception):
                        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {video_id}: {result}")
                        continue
                    if result:
                        results.append(result)

            index += batch_size
            if index < total:
                print(f"‚è≥ –ñ–¥—ë–º {delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–∞—á–∫–æ–π –∑–∞–ø—Ä–æ—Å–æ–≤ ({index}/{total})")
                await asyncio.sleep(delay)

        return results

    async def download_image(self, url: str, proxy: str = None) -> Union[bytes, None]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å YouTube (–º–æ–∂–Ω–æ —Å –ø—Ä–æ–∫—Å–∏)."""
        formatted_proxy = self.prepare_proxy(proxy)
        proxies = {"http": formatted_proxy, "https": formatted_proxy} if formatted_proxy else None

        def _download() -> bytes:
            response = requests.get(url, timeout=20.0, proxies=proxies)
            response.raise_for_status()
            return response.content

        try:
            return await asyncio.to_thread(_download)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {url}: {e}")
            return None

    async def upload_image(self, video_id: int, image_url: str, proxy: str = None):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—Å –ø—Ä–æ–∫—Å–∏) –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –µ–≥–æ –Ω–∞ API, –Ω–æ—Ä–º–∞–ª–∏–∑—É—è –ø—É—Ç—å."""
        image_bytes = await self.download_image(image_url, proxy=proxy)
        if not image_bytes:
            return None, "Download failed"

        file_name = image_url.split("/")[-1].split("?")[0] or "cover.jpg"
        files = {"file": (file_name, image_bytes, "image/jpeg")}

        def _upload():
            response = requests.post(
                f"http://127.0.0.1:8000/api/v1/videos/{video_id}/upload-image/",
                files=files,
                timeout=30.0,
            )
            response.raise_for_status()
            try:
                payload = response.json()
            except ValueError:
                payload = {}
            return response.status_code, payload

        try:
            status_code, payload = await asyncio.to_thread(_upload)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–æ—Ç–æ –¥–ª—è –≤–∏–¥–µ–æ {video_id}: {e}")
            return None, str(e)

        image_path = None
        if isinstance(payload, dict):
            image_path = payload.get("image")
            if image_path and not image_path.startswith(("http://", "https://", "/")):
                image_path = "/" + image_path

        return status_code, image_path or payload

    def extract_article_tag(self, caption: str) -> Optional[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É —Å–æ –í–°–ï–ú–ò –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏ (#sv, #jw –∏ —Ç.–¥.) —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –∏–ª–∏ None."""
        if not caption:
            return None

        allowed_tags = ["#sv", "#jw", "#qz", "#sr", "#fg"]
        found_tags = []

        caption_lower = caption.lower()
        original_caption = caption  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è

        for tag in allowed_tags:
            if tag in caption_lower:
                # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                start = caption_lower.find(tag)
                if start != -1:
                    # –ë–µ—Ä—ë–º —Ç–æ—á–Ω–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –∫—Ç–æ-—Ç–æ –Ω–∞–ø–∏—Å–∞–ª #SV)
                    exact_tag = original_caption[start:start + len(tag)]
                    found_tags.append(exact_tag)

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        found_tags = sorted(set(found_tags))

        return ",".join(found_tags) if found_tags else None

    async def parse_channel(self, url: str, channel_id: int, user_id: int, max_retries: int = 3, proxy_list: list = None):
        """
        –ù–æ–≤–∞—è –ª–æ–≥–∏–∫–∞:
        1. –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –∏–∑ —à–∞–ø–∫–∏ –∫–∞–Ω–∞–ª–∞.
        2. –°–∫—Ä–æ–ª–ª–∏–º –ª–µ–Ω—Ç—É —à–æ—Ä—Ç–æ–≤ –¥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏–±–æ –¥–æ –∫–æ–Ω—Ü–∞.
        3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –∏ –ø—Ä–µ–≤—å—é –∏–∑ DOM.
        4. –ü–æ –æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∞ –ø—Ä–æ–∫—Å–∏ —Å–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ httpx + BS4.
        5. –ü–µ—Ä–µ–¥–∞—ë–º –¥–∞–Ω–Ω—ã–µ –¥–∞–ª—å—à–µ –Ω–∞ API (–Ω–∏–∂–µ –ø–æ —Ñ—É–Ω–∫—Ü–∏–∏).
        """
        self.proxy_list = proxy_list or []
        current_proxy_index = 0

        if not url.endswith('/shorts'):
            url = url.rstrip('/') + '/shorts'
        print(f"–ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –∫–∞–Ω–∞–ª: {url}")

        async def get_proxy_config(proxy_str):
            try:
                if "@" in proxy_str:
                    auth, host_port = proxy_str.split("@", 1)
                    username, password = auth.split(":")
                    host, port = host_port.split(":")
                    return {"server": f"http://{host}:{port}", "username": username, "password": password}
                else:
                    host, port = proxy_str.split(":")
                    return {"server": f"http://{host}:{port}"}
            except Exception as e:
                print(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–∫—Å–∏: {e}")
                return None

        async def create_browser_with_proxy(proxy_str, playwright):
            proxy_config = await get_proxy_config(proxy_str) if proxy_str else None
            browser = await playwright.chromium.launch(
                headless=False,
                args=[
                    "--headless=new",
                    "--disable-blink-features=AutomationControlled",
                    "--start-maximized"
                ],
            )
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                proxy=proxy_config
            )
            page = await context.new_page()
            return browser, context, page

        proxy_candidates = list(self.proxy_list) if self.proxy_list else [None]
        if self.proxy_list:
            random.shuffle(proxy_candidates)
        max_proxy_attempts = (
            min(max_retries, len(proxy_candidates)) if self.proxy_list else max(1, max_retries)
        )
        max_proxy_attempts = max(1, max_proxy_attempts)

        all_videos_data: List[Dict] = []
        header_videos_count: Optional[int] = None
        total_videos_from_dom = 0
        videos_limit = 0
        total_collected = 0

        best_state = None
        best_total = 0

        for attempt_idx in range(max_proxy_attempts):
            current_proxy = proxy_candidates[attempt_idx % len(proxy_candidates)] if proxy_candidates else None
            print(
                f"üîÅ –ü–æ–ø—ã—Ç–∫–∞ {attempt_idx + 1}/{max_proxy_attempts} "
                f"—Å –ø—Ä–æ–∫—Å–∏ {current_proxy or '–±–µ–∑ –ø—Ä–æ–∫—Å–∏'}"
            )
            self.reset_dom_state()

            playwright = None
            browser = None
            context = None
            page = None

            try:
                playwright = await async_playwright().start()
                browser, context, page = await create_browser_with_proxy(current_proxy, playwright)

                print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É Shorts‚Ä¶")
                await page.goto(url, wait_until="networkidle", timeout=60000)

                cookie_selectors = [
                    "button[aria-label='Accept all']",
                    "button:has-text('Accept all')",
                    "button:has-text('–ü—Ä–∏–Ω—è—Ç—å –≤—Å–µ')",
                    "button:has-text('–ü—Ä–∏–Ω—è—Ç—å –≤—Å—ë')",
                    "ytd-button-renderer#accept-button button",
                ]
                for selector in cookie_selectors:
                    try:
                        btn = await page.query_selector(selector)
                        if btn:
                            await btn.click()
                            await page.wait_for_timeout(1200)
                            print("–ó–∞–∫—Ä—ã—Ç–∞ –º–æ–¥–∞–ª–∫–∞ —Å –∫—É–∫–∏")
                            break
                    except Exception:
                        continue

                header_videos_count = await self.get_videos_count_from_header(page)
                if header_videos_count:
                    print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –∏–∑ —à–∞–ø–∫–∏: {header_videos_count}")
                else:
                    print("‚ÑπÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –∏–∑ —à–∞–ø–∫–∏, –æ–ø–∏—Ä–∞–µ–º—Å—è –Ω–∞ DOM.")

                selector = "ytd-rich-item-renderer, ytd-reel-item-renderer, ytm-shorts-lockup-view-model, ytd-grid-video-renderer"
                total_videos_from_dom = await self.scroll_until(
                    page,
                    url,
                    selector=selector,
                    target_count=header_videos_count,
                    delay=3.0
                )
                print(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ –≤ DOM: {len(self.dom_order)} (scroll_until –≤–µ—Ä–Ω—É–ª {total_videos_from_dom})")

            except Exception as main_error:
                print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å Playwright: {main_error}")
            finally:
                for obj, name in [(page, "page"), (context, "context"), (browser, "browser"), (playwright, "playwright")]:
                    if obj:
                        try:
                            if name == "playwright":
                                await obj.stop()
                            else:
                                await obj.close()
                        except Exception as e:
                            print(f"–û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è {name}: {e}")

            total_collected = len(self.dom_order)
            if total_collected > best_total:
                best_state = {
                    "dom_images": dict(self.dom_images),
                    "dom_video_links": dict(self.dom_video_links),
                    "dom_order": list(self.dom_order),
                    "header_count": header_videos_count,
                    "total_from_dom": total_videos_from_dom,
                }
                best_total = total_collected

            if total_collected == 0:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ –≤ —ç—Ç–æ–π –ø–æ–ø—ã—Ç–∫–µ, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –ø—Ä–æ–∫—Å–∏.")
                await asyncio.sleep(1.5)
                continue

            if (
                header_videos_count
                and total_collected < header_videos_count
                and attempt_idx + 1 < max_proxy_attempts
            ):
                print(
                    f"‚ö†Ô∏è –°–æ–±—Ä–∞–Ω–æ —Ç–æ–ª—å–∫–æ {total_collected} –∏–∑ {header_videos_count} –≤–∏–¥–µ–æ. "
                    "–ü—Ä–æ–±—É–µ–º –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –¥—Ä—É–≥–∏–º –ø—Ä–æ–∫—Å–∏."
                )
                await asyncio.sleep(1.0)
                continue

            break
        else:
            if best_state and best_state["dom_order"]:
                print("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –ª—É—á—à–µ–π –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.")
                self.dom_images = best_state["dom_images"]
                self.dom_video_links = best_state["dom_video_links"]
                self.dom_order = best_state["dom_order"]
                header_videos_count = best_state["header_count"]
                total_videos_from_dom = best_state["total_from_dom"]
                total_collected = len(self.dom_order)
            else:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∏–∑ DOM –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫.")
                return []
        total_collected = len(self.dom_order)
        if total_collected == 0:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∏–∑ DOM.")
            return []

        videos_limit = header_videos_count if header_videos_count else total_collected
        videos_limit = min(videos_limit, total_collected)
        videos_to_process = self.dom_order[:videos_limit]
        print(
            f"üéØ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ {len(videos_to_process)} –≤–∏–¥–µ–æ "
            f"(—à–∞–ø–∫–∞: {header_videos_count or '‚Äî'}, —Å–æ–±—Ä–∞–Ω–æ: {total_collected})"
        )

        metadata_list = await self.fetch_videos_with_proxies(videos_to_process)
        print(f"üì¶ –ü–æ–ª—É—á–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(metadata_list)} –≤–∏–¥–µ–æ")

        for meta in metadata_list:
            video_id = meta.get("video_id")
            if not video_id:
                continue
            image_url = self.dom_images.get(video_id) or f"https://i.ytimg.com/vi/{video_id}/hqdefault.jpg"
            description = meta.get("description") or ""
            articles = self.extract_article_tag(description)
            all_videos_data.append(
                {
                    "link": meta.get("link"),
                    "type": "youtube",
                    "name": meta.get("title") or "",
                    "image": image_url,
                    "channel_id": channel_id,
                    "articles": articles,
                    "amount_views": meta.get("views") or 0,
                    "amount_likes": meta.get("likes") or 0,
                    "amount_comments": meta.get("comments") or 0,
                    "date_published": meta.get("published_at") or "",
                }
            )

        print(
            f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(all_videos_data)} –∏–∑ {videos_limit} –≤–∏–¥–µ–æ "
            f"(DOM –Ω–∞–π–¥–µ–Ω–æ: {total_collected})"
        )
        processed_count = 0
        image_queue = []
        queued_video_ids = set()
        for video_data in all_videos_data:
            try:
                async with httpx.AsyncClient(timeout=20.0) as client:
                    # print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∏–¥–µ–æ: {video_data['link']}")
                    check_resp = await client.get(f"http://127.0.0.1:8000/api/v1/videos/?link={video_data['link']}")
                    is_new = False
                    video_id = None

                    if check_resp.status_code == 200:
                        res = check_resp.json()
                        vids = res.get("videos", [])
                        if vids:
                            existing_video = vids[0]
                            video_id = existing_video['id']
                            update_payload = {
                                "amount_views": video_data["amount_views"],
                                "amount_likes": video_data["amount_likes"],
                                "amount_comments": video_data["amount_comments"],
                                "date_published": video_data["date_published"],
                                "articles": video_data["articles"],
                            }
                            await client.patch(
                                f"http://127.0.0.1:8000/api/v1/videos/{video_id}",
                                json=update_payload
                            )

                            existing_image = existing_video.get("image")
                            image_missing = not existing_image
                            image_needs_update = False
                            if isinstance(existing_image, str):
                                normalized_existing = existing_image.strip()
                                if normalized_existing.startswith(("http://", "https://")):
                                    image_needs_update = True
                            if (image_missing or image_needs_update) and video_data.get("image"):
                                if video_id not in queued_video_ids:
                                    image_queue.append((video_id, video_data["image"]))
                                    queued_video_ids.add(video_id)
                        else:
                            is_new = True
                    else:
                        is_new = True

                    if is_new:
                        resp = await client.post("http://127.0.0.1:8000/api/v1/videos", json=video_data)
                        resp.raise_for_status()
                        video_id = resp.json()["id"]
                        # print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤–æ–µ –≤–∏–¥–µ–æ {video_id}")
                        if video_data.get("image") and video_id not in queued_video_ids:
                            image_queue.append((video_id, video_data["image"]))
                            queued_video_ids.add(video_id)
                processed_count += 1
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {video_data.get('link')}: {e}")

        print(f"üì¶ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –≤–∏–¥–µ–æ, –æ–∂–∏–¥–∞—é—Ç –∑–∞–≥—Ä—É–∑–∫–∏ {len(image_queue)} –æ–±–ª–æ–∂–µ–∫.")

        # --- –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ---
        idx = 0
        while idx < len(image_queue):
            proxy = proxy_list[current_proxy_index] if proxy_list else None
            current_proxy_index = (current_proxy_index + 1) % len(proxy_list) if proxy_list else 0
            batch = image_queue[idx:idx + 15]
            print(f"üñºÔ∏è –ó–∞–≥—Ä—É–∂–∞–µ–º {len(batch)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ {proxy or '–±–µ–∑ –ø—Ä–æ–∫—Å–∏'}")

            for vid, img_url in batch:
                try:
                    status, _ = await self.upload_image(vid, img_url, proxy=proxy)
                    print(f"{'‚úÖ' if status == 200 else '‚ö†Ô∏è'} –§–æ—Ç–æ –¥–ª—è –≤–∏–¥–µ–æ {vid} ‚Üí —Å—Ç–∞—Ç—É—Å {status}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–æ—Ç–æ {vid}: {e}")
                await asyncio.sleep(5.0)
            idx += 15

        print(f"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω: {processed_count} –≤–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ.")


# ----------------------- –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ -----------------------

async def main():
    proxy_list = [
        "6hro8o:N6A7Yn@181.177.84.234:9413",
        "6hro8o:N6A7Yn@181.177.87.128:9966",
        "6hro8o:N6A7Yn@181.177.84.125:9613",
        "6hro8o:N6A7Yn@23.236.139.90:9758",
        "6hro8o:N6A7Yn@23.236.141.118:9234",
        "6hro8o:N6A7Yn@23.236.141.94:9893",
        "6hro8o:N6A7Yn@23.236.138.18:9055",
        "6hro8o:N6A7Yn@23.236.149.166:9775",
        "6hro8o:N6A7Yn@23.236.148.87:9845",
        "6hro8o:N6A7Yn@170.246.55.141:9663",
        "6hro8o:N6A7Yn@191.102.172.185:9891",
        "6hro8o:N6A7Yn@191.102.172.131:9083",
        "6hro8o:N6A7Yn@170.246.55.97:9246",
    ]
    parser = ShortsParser()
    url = "https://www.youtube.com/@nastya.beomaa"
    user_id = 1
    await parser.parse_channel(url, channel_id=5, user_id=user_id, proxy_list=proxy_list)


if __name__ == "__main__":
    asyncio.run(main())
